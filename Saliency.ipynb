import torch
from transformers import BertTokenizer, BertModel
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import pandas as pd
import random
from nltk.corpus import gutenberg, stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.probability import FreqDist
from nltk import pos_tag, ne_chunk

# Function to compute the saliency score using BERT embeddings
def compute_saliency_using_embeddings(word, sentence, model, tokenizer):
    # Tokenize sentence with and without the word
    tokens_with_word = tokenizer(sentence, return_tensors="pt")
    tokens_without_word = tokenizer(sentence.replace(word, ''), return_tensors="pt")

    # Get embeddings
    with torch.no_grad():
        embeddings_with_word = model(**tokens_with_word).last_hidden_state.mean(dim=1)
        embeddings_without_word = model(**tokens_without_word).last_hidden_state.mean(dim=1)

    # Compute saliency as difference between embeddings
    saliency_score = torch.nn.functional.cosine_similarity(embeddings_with_word, embeddings_without_word)
    return 1 - saliency_score.item()

# Initialize BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()

# Get sentences from Gutenberg corpus and prepare data as before
sentences = gutenberg.sents()
text = [" ".join(sent) for sent in sentences]
random_sentences = random.sample(text, 1000)
tokenized_sentences = [word_tokenize(sent) for sent in random_sentences]
all_words = [word for sent in tokenized_sentences for word in sent]
frequency_dict = FreqDist(all_words)
stop_words = set(stopwords.words('english'))

# Create dataset with the new saliency computation
dataset = []
for sent in tokenized_sentences:
    sent_text = " ".join(sent)
    for word in sent:
        if word.isalpha() and word.lower() not in stop_words:  # Only compute for meaningful words
            named_entities = ne_chunk(pos_tag([word]))
            is_named_entity = "Yes" if hasattr(named_entities[0], 'label') else "No"
            saliency_score = compute_saliency_using_embeddings(word, sent_text, model, tokenizer)
            dataset.append([word, saliency_score, is_named_entity])

# Visualization and DataFrame creation is retained with some changes
df = pd.DataFrame(dataset, columns=['Word', 'Saliency_Score', 'Is_Named_Entity'])

# Word Cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frequency_dict)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Word Frequencies')
plt.show()

# Histogram of Saliency Scores
plt.figure(figsize=(10, 6))
sns.histplot(df['Saliency_Score'], kde=True)
plt.title('Distribution of Saliency Scores')
plt.xlabel('Saliency Score')
plt.ylabel('Frequency')
plt.show()

# Bar plot of top 10 salient words
top_salient_words = df.groupby('Word')['Saliency_Score'].mean().nlargest(10)
plt.figure(figsize=(10, 6))
top_salient_words.plot(kind='bar')
plt.title('Top 10 Salient Words')
plt.ylabel('Saliency Score')
plt.xlabel('Word')
plt.show()
